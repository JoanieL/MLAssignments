{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your name:\n",
    "\n",
    "<pre> Joan Soo Li Lim </pre>\n",
    "\n",
    "### Collaborators:\n",
    "\n",
    "<pre> None </pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown reference can be found here:\n",
    "    http://nestacms.com/docs/creating-content/markdown-cheat-sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Why would it be a problem if our training set and test set are the same.\n",
    "\n",
    "The end goal of ML is to build a model that can be generalized to new data. It would be beneficial to divide the training data into a training set and test/validation set, where the latter set represents real world data (i.e. unseen and unfitted data). Otherwise, if the training data  is reused during test/validation, then one will not be able to predict the accuracy of the model on data it has not seen before. In other words, if we train on test/validation data, then the model will appear to fit with good accuracy (as we are testing on the same data) but might not be able to generalize adequately when the ML model encounters new data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Explain step by step the process to select k in the k-nearest neighbor algorithm (pseudocode)\n",
    "\n",
    "To find k using the simpliest algorithm (k with lowest error rate), the pseudocode is as follows: \n",
    "\n",
    "Find K-simple\n",
    "1. Create an array err_rates of size n and initialize to 0 (n = number of observations in training set) **\n",
    "2. Set the random seed\n",
    "3. Split the number of observations into 2 sets: training (i.e. 80%) and validation (i.e. 20%)  \n",
    "4. For each k in range of 1 .. square root(n) \n",
    "    a. Fit the model on the training data for k neighbours\n",
    "    b. Predict the value of the validation set from the model derived\n",
    "    c. Determine the error rate of validation set vs. predicted value (using something like MSE)\n",
    "    d. Store the error rate in err_rates at index i == k\n",
    "5. Find the minimum value of err_rates and return its index.     \n",
    "\n",
    "\n",
    "Note, if we select k based on M fold validation, the pseudocode will now be encapsulated by M iterations such as:\n",
    "\n",
    "Find K-Mfold\n",
    "1. Create a matrix for cross validation errrors cross_err_rates of size n by M \n",
    "    a. Initialize to 0 \n",
    "    b. Add ID column = k for range 1 .. square root(n)\n",
    "    c. Add error_mean column to store mean of error for this k\n",
    "2. Set the random seed\n",
    "3. For each m in range of 1 .. M\n",
    "        For each k in range of 1 .. square root(n)  \n",
    "        a. Split the number of observations into 2 sets: training (i.e. mâˆ’1/m %) and validation (i.e. 1/m %)\n",
    "        b. Fit the model on the training data for k neighbours\n",
    "        c. Predict the value of the validation set from the model derived\n",
    "        d. Determine the error rate of validation set vs. predicted value (using something like MSE)\n",
    "        e. Store the error rate in cross_err_rates at location (k, m)\n",
    "        f. Find the average value of cross_err_rates for this m and store in error_mean\n",
    "4. Find the minimum value of error_mean in cross_err_rates and return the value of ID column for that row        \n",
    "        \n",
    "\n",
    "** The bound for k is set as per lecture slides (Lecture 1 slide 60).\n",
    "\n",
    "Reference:\n",
    "https://idc9.github.io/stor390/notes/cross_validation/cross_validation.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. For the k-nearest regression. What happens when k = n. Where n is equal to the training size.\n",
    "\n",
    "In general for a large dataset, a k that is too small may result in overfitting (i.e extreme case k==1 considers each point individually) . Conversely, a large k may result in smoothling out noise (outliers) and eliminating important details. If k==n, we have the extreme case where the whole distribution is evened out and as per the next Q below for KNN regression, we will then take the average of all the points. This averaging will lead to a straight line (high bias, no variance). In the case of classification, we will then simply take the most dominant class (i.e. If there are 2 classes, where class X appears 10 times and class Y appears 9 times, the model will return class X).\n",
    "\n",
    "Reference:\n",
    "http://www.cs.bham.ac.uk/~jxb/INC/l9.pdf (slide 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Define a function that takes a 1-d numpy array, a parameter k, and a number p.\n",
    "The function returns an estimate equal to the mean of the closest k points to the number p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbor(input_data, k, p):\n",
    "    \"\"\"Returns the k-neighbor estimate for p using data input_data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    input_data -- numpy array of all the data\n",
    "    k -- Number of k\n",
    "    p -- input values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initalize\n",
    "    mean = 0\n",
    "    distances = []\n",
    "    neighbors = []\n",
    "    \n",
    "    # For each point in input_data, compute Manhattan distance to p\n",
    "    for x in range(len(input_data)):\n",
    "        dist = np.abs(p - input_data[x])\n",
    "        distances.append((input_data[x], dist))\n",
    "    \n",
    "    # Sort the distances \n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    \n",
    "    # Get the first k neighbours\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    \n",
    "    # Compute the mean of the first k neighbours' value(s) in input_data\n",
    "    mean = np.mean(neighbors)\n",
    "    \n",
    "    # Return the mean\n",
    "    return (mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "14.0\n",
      "26.0\n",
      "40.0\n",
      "31.333333333333332\n",
      "19.6\n"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "data = np.array([1,3,4,5,7,8,11,12,13,15,19,24,25,29,40])\n",
    "print(k_neighbor(input_data=data, k=3, p=5))\n",
    "print(k_neighbor(input_data=data, k=2, p=15))\n",
    "print(k_neighbor(input_data=data, k=3, p=25))\n",
    "print(k_neighbor(input_data=data, k=1, p=55))\n",
    "print(k_neighbor(input_data=data, k=3, p=55))\n",
    "print(k_neighbor(input_data=data, k=10, p=55))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations include: \n",
    "\n",
    "For k=3, p=5 -> we have the case where both 3 and 7 have a distance of 2 away from 5. Therefore, depending on the algorithmic choice, we could have picked either 3 or 7 when attempting to predict the value for p. As I chose to sort by original array range, I picked 3. The predicted value is then 4.0. If some other partitioning was chosen to sort and order the data, the value could be 5.3 (4+5+7 / 3).\n",
    "\n",
    "For p=55 -> depending on the k, we have a huge variance of predicted values. The is b/c the original dataset has data that is primarily on the lower end but sparse for higher p. Therefore, for high p, the model cannot generalize as well as for low p.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Similar to Q4 but for the n dimentional case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(a,b):\n",
    "    \"\"\"Returns the l1 norm (a,b)\"\"\"\n",
    "    return np.sum(abs(a-b))\n",
    "\n",
    "    \n",
    "def l2_norm(a,b):\n",
    "    \"\"\"Returns the l2 norm (a,b)\"\"\" \n",
    "    return np.sum((b-a)**2)**0.5\n",
    "\n",
    "    \n",
    "def k_neighbor_nd(input_data, k, p, metric='l1', mode='mean'):\n",
    "    \"\"\"Returns the k-neighbor estimate for p using data input_data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    input_data -- numpy array of all the data\n",
    "    k -- Number of k\n",
    "    p -- input values\n",
    "    metric -- l1 or l2. l1 norm or l2 norm https://en.wikipedia.org/wiki/Norm_(mathematics)\n",
    "    mode -- estimator possible values = 'mean', 'median', 'max'\n",
    "    \n",
    "    Implement the l1 and l2 norms\n",
    "    for mean, median and max, use np.mean, np.median and np.max\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    arr = np.zeros (0)\n",
    "    idx = []\n",
    "    neighbors = []\n",
    "    predicted = []\n",
    "    debug = False\n",
    "    \n",
    "    # For each point in input_data, compute distance to p using either l1 or l2\n",
    "    if metric == 'l1':\n",
    "        for i in range (data_4d.shape[0]):\n",
    "           arr = np.append(arr, l1_norm (p, data_4d[i]))\n",
    "    else :\n",
    "        for i in range (data_4d.shape[0]):\n",
    "           arr = np.append(arr, l2_norm (p, data_4d[i])) \n",
    "    \n",
    "    # Sort the distances and get the first k neighbours's value(s)\n",
    "    idx = np.argpartition (arr, k) # indices of input_data of first k \n",
    "    neighbors = (data_4d[idx[:k]])\n",
    "    \n",
    "    if debug:    \n",
    "        print (arr)\n",
    "        print (idx)\n",
    "        print (neighbors)\n",
    "    \n",
    "    # Compute the predictor of the first k neighbours' value(s) in input_data using appropriate mode\n",
    "    if mode == 'mean':\n",
    "        predicted = np.mean(neighbors, axis = 0)   \n",
    "    elif mode == 'max':\n",
    "        predicted = np.max(neighbors, axis = 0)\n",
    "    else :\n",
    "        predicted = np.median(neighbors, axis = 0)\n",
    "    \n",
    "    # Return the predictor\n",
    "    return predicted\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_4d = np.array([[4, 1, 2, 1], [1, 4, 2, 0], [3, 3, 1, 1], \n",
    "        [4, 0, 0, 0], [1, 2, 0, 0], [3, 4, 2, 3], \n",
    "        [2, 4, 4, 2], [2, 1, 4, 1], [3, 3, 2, 4], \n",
    "        [4, 3, 0, 4], [2, 2, 4, 0],[4, 3, 0, 2], \n",
    "        [4, 3, 0, 2], [0, 3, 4, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.         2.33333333 4.         1.        ]\n",
      "[4. 3. 0. 2.]\n",
      "[4 4 2 4]\n",
      "[3. 3. 2. 4.]\n",
      "[3. 4. 2. 3.]\n",
      "--------------------------\n",
      "[1.33333333 2.         4.         1.        ]\n",
      "[3.5 3.  0.5 1.5]\n",
      "[4 4 2 4]\n",
      "[3. 3. 2. 4.]\n",
      "[3. 4. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 1, 4, 3], metric='l1', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=2, p=[4, 4, 0, 0], metric='l1', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 2, 2, 4], metric='l1', mode='max'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=1, p=[2, 3, 3, 4], metric='l1', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 3, 3, 4], metric='l1', mode='median'))\n",
    "print (\"--------------------------\") # Separate metric\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 1, 4, 3], metric='l2', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=2, p=[4, 4, 0, 0], metric='l2', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 2, 2, 4], metric='l2', mode='max'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=1, p=[2, 3, 3, 4], metric='l2', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 3, 3, 4], metric='l2', mode='median'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations include:\n",
    "\n",
    "Regardless of metric, the last 3 examples for either l1 or l2 norm return identical k neighbors. For the max, we have a predicted value that still has quite a distance from p. I suspect there could be a larger variance for large k for this mode. In terms of p = [2, 3, 3, 4] and k = 1, we simply return the closest neighbor and hence the mode is not really relevant. However, for k = 3, the predicted value with mode = median does not really provide a better error rate as the additional neighbors have quite a distance from p. \n",
    "\n",
    "For the first 2 examples, the metric l2 computes a more complex set of distances and returns one neighbor that is different from the metric l1. In general for these examples, l1 seems to favor neighbors with distances that minimize differences amongst each dimension, whereas l2 seems to favor neighbors with distances that minimize differences as a whole amongst all the dimensions due to errors > 1 being squared. Interestingly, if we look at the error rate between p and the predicted value, then l1 seems to be slightly better. Since I am a novice, I am not sure if higher dimensions affect KNN negatively due to this sort of distortion in the distance calculation. Perhaps dimension reduction could help. Or perhaps the l2 model could have been tweaked by picking a higher k (in order to have more neighbors to average / smooth out the predicted value). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Read the documentation on KNeighborsRegressor\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "    \n",
    "Explore the source code:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/ef5cb84a/sklearn/neighbors/regression.py\n",
    "        \n",
    "How different it is from your implementation? How well can you follow the code? Did you learn something new?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has various portions that are difficult to follow, but overall the main idea is to permit additional tweaking on the KNN algorithm beyond that of metric and mode. In particular, there are possibilities such as adding weights (either inverse to the distance or user-defined) to data points such that those of a further distance could have less bearing on the model's fit. This notion would be beneficial for areas of a distribution that have sparse data points around a specific p (as noted in our example above). The data structures utilized for speed and memory optimization are far more advanced than my arrays and are especially needed for higher dimensions and larger input. Note, I suspect that building n-leaf trees could cause me to have a massive headache. Additionally, the default distance computed is Minkowski which generalized to p = 1 (l1) and p = 2 (l2). I am not sure what the results would be for higher powers of p and its relvance as it approaches infinity, but perhaps this will be covered later.  There are other minor details such as the ability to return the distances and indices / values of the neighbors (which I have implemented indirectly via a debug boolean). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
