{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your name:\n",
    "\n",
    "Joan Soo Li Lim\n",
    "\n",
    "### Collaborators:\n",
    "\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the housing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "HOUSING_PATH = \".\"\n",
    "\n",
    "def fetch_housing_data(housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build full pipeline for the data analysis following the example of the notebook.\n",
    " Hint: the main part requested to change is the algorithm used (Lasso regression)\n",
    "\n",
    "If you want to learn more about the Lasso regression, see resources below:\n",
    "- http://scikit-learn.org/stable/modules/linear_model.html#lasso\n",
    "- https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations for building pipeline:\n",
    "\n",
    "- Split data into training and testing sets below.\n",
    "- Convert all categorical data to one-hot vectors below\n",
    "- Normalize all non-categorical data \n",
    "-  Perform Lasso-based regression using a variety of values for $\\alpha$ between 0 and 1 via a grid search where  *housing_labels* is the output and all other features are the input (similar to as seen in lecture two.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude             20640\n",
       "latitude              20640\n",
       "housing_median_age    20640\n",
       "total_rooms           20640\n",
       "total_bedrooms        20433\n",
       "population            20640\n",
       "households            20640\n",
       "median_income         20640\n",
       "median_house_value    20640\n",
       "ocean_proximity       20640\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values. \n",
    "housing.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only \"total_bedrooms\" column has null values. Deal with this now. Fill the null with the median value.\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: One hot vectors for categorical\n",
    "# Step 2: Normalize numerical data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn import preprocessing\n",
    "\n",
    "debug = False\n",
    "\n",
    "# Split data into training and testing sets using sklearn build-in function. \n",
    "# As per example from the text, use the same defaults of ratio = 20% and random state = 42. \n",
    "data_train, data_test = train_test_split(housing, \n",
    "                                         test_size=0.2, \n",
    "                                         random_state=42)\n",
    "  \n",
    "# Create labels for training set    \n",
    "housing_train = data_train.drop(\"median_house_value\", axis=1) \n",
    "housing_train_labels = data_train[\"median_house_value\"].copy() \n",
    "\n",
    "if (debug):\n",
    "    print(len(data_train), \"train +\", len(data_test), \"test\")\n",
    "    print (data_train.head())\n",
    "    print (data_test.head())\n",
    "    print (\"------------------------\")\n",
    "    print (housing_train.head())\n",
    "    print (housing_train_labels.head())\n",
    "    \n",
    "# First step: Convert categorical data (ocean proximity) to one-hot vectors\n",
    "# sklearn 2.0 is still in DEV so I chose LabelBinarizer instead.\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# Fit based on entire set to get ALL labels\n",
    "lb.fit (housing[['ocean_proximity']]) \n",
    "\n",
    "# Transform train and test sets separately\n",
    "train_cat_1hot = lb.transform(housing_train[['ocean_proximity']])\n",
    "test_cat_1hot = lb.transform(data_test[['ocean_proximity']])\n",
    "\n",
    "if (debug):\n",
    "    print (lb.classes_)\n",
    "    print (train_cat_1hot[:5])\n",
    "    print (test_cat_1hot[:5])\n",
    "    \n",
    "# Second step: Normalize non-categorical data using Euclidean distance\n",
    "housing_num = housing_train.select_dtypes(include=[np.number])\n",
    "housing_num_normalized = preprocessing.normalize(housing_num, norm='l2')\n",
    "\n",
    "if (debug):\n",
    "    print (housing_num.head())\n",
    "    print (housing_num_normalized[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-117.03,   32.71,   33.  , ...,    0.  ,    0.  ,    1.  ],\n",
       "       [-118.16,   33.77,   49.  , ...,    0.  ,    0.  ,    1.  ],\n",
       "       [-120.48,   34.66,    4.  , ...,    0.  ,    0.  ,    1.  ],\n",
       "       ...,\n",
       "       [-118.38,   34.03,   36.  , ...,    0.  ,    0.  ,    0.  ],\n",
       "       [-121.96,   37.58,   15.  , ...,    0.  ,    0.  ,    0.  ],\n",
       "       [-122.42,   37.77,   52.  , ...,    0.  ,    1.  ,    0.  ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, using pipeline. I left the above as-is so that I can see what is happening step by step. Cheerios.\n",
    "# Majority of the code is from the lecture notebook with changes identified in comments.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]   \n",
    "\n",
    "# Numerical pipeline  \n",
    "# NOTE:\n",
    "# (1) Use median strategy for NA values. \n",
    "# (2) Did not combine any other attributes as it wasn't specified in the assignment.\n",
    "# (3) Did not use StandardScaler as we will normalize using parameter into Lasso later. \n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', Imputer(strategy=\"median\")),])\n",
    "\n",
    "# Categorical pipeline \n",
    "# NOTE:\n",
    "# (1) Perform Labeled Encoding first as input to (2) must be numerical value as per version 19.X. \n",
    "# (2) Use One Hot Encoder without a sparse matrix. \n",
    "\n",
    "le = LabelEncoder ()\n",
    "\n",
    "# OMIT. Save directly into ocean_proximity --- housing_train[['ocean_encoded']] = housing_train[['ocean_proximity']].copy() #\n",
    "# Encode ocean proximity with values 0 to 4. Use full set to ensure we get all labels.\n",
    "housing[['ocean_proximity']] = housing[['ocean_proximity']].apply(le.fit)\n",
    "housing_train[['ocean_proximity']] = housing_train[['ocean_proximity']].apply(le.transform)\n",
    "\n",
    "if (debug):\n",
    "    print (data_train[\"ocean_proximity\"].value_counts())\n",
    "    print (housing_train[\"ocean_proximity\"].value_counts())\n",
    "    print (housing_train.head())\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False)),])\n",
    "\n",
    "   \n",
    "# Full pipeline    \n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing_train)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "68637.22420908543\n",
      "------------\n",
      "68643.35988805693 {'alpha': 1.0}\n",
      "68639.44250156602 {'alpha': 0.7}\n",
      "68637.81752886708 {'alpha': 0.5}\n",
      "68637.32710598563 {'alpha': 0.4}\n",
      "68637.22420908543 {'alpha': 0.1}\n",
      "68637.50618326574 {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Perform Lasso regression for values α between 0 and 1 via a grid search \n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# A range of alpha values to test from 1 to 0\n",
    "# I omitted 0 as it was not converging despite increasing iterations to 10000 (noted below)\n",
    "alphas = np.array([1,0.7,0.5,0.4,0.1,0.01])\n",
    "\n",
    "# Normalize any numerical values and try max_iter of 10000\n",
    "lasso = Lasso(normalize=True, max_iter=10000)\n",
    "\n",
    "# Try 5 folds as increasing to 10 will yield convergence problems\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid=dict(alpha=alphas) , cv=5,\n",
    "                           scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_train_labels)\n",
    "\n",
    "if (debug):\n",
    "    print(grid_search)\n",
    "    \n",
    "# Results of the grid search for best alpha\n",
    "print(grid_search.best_estimator_.alpha)\n",
    "rmse = np.sqrt(-grid_search.best_score_)\n",
    "print (rmse)\n",
    "print (\"------------\")\n",
    "\n",
    "# Results of the grid search in general\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70052.57839145602\n"
     ]
    }
   ],
   "source": [
    "# Find RSME of test set.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# Perform same preprocessing\n",
    "X_test = data_test.drop(\"median_house_value\", axis=1)\n",
    "y_test = data_test[\"median_house_value\"].copy()\n",
    "X_test[['ocean_proximity']] = X_test[['ocean_proximity']].apply(le.transform)\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)   \n",
    "\n",
    "print (final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Why is it necessary to normalize all continuous variables before performing Lasso? (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not have time, or else I would do some research. My guess is that since Lasso aims to penalize the coefficients, then it would mean that a larger coefficient on a different scale (i.e. not normalized) will be penalized more than another coefficient with a smaller scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "For what values of $\\alpha$ does Lasso perform best? Does it perform as well on the housing data as the linear regressor from the lectures? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment:\n",
    "Personally, I don't think I tuned this well nor did I execute all steps as per Module 2 notebook. However, I am out of time. The errors arising from the provided examples were time consuming as I searched for 'stable' versions of APIs I could use. I prefer not to install / use 'unstable' code. Also, I did this within a span of a day pluw so as to submit on Thursday! As mentioned via email, I could not work on this prior due to health issues. I look forward to the solution discussion, after which I will return and tune this out a bit more. I suspect I will learn a great deal. :) \n",
    "\n",
    "Answer:\n",
    "For my hacky grid search, the best alpha was 0.1 with a RSME of 68637.22420908543. It's final RSME of the entire test set is 70052.57839145602. I don't think it is fair to compare to the linear regressor (with a smaller final RMSE of 68628.19819848922 for 5 data points as noted in cell 80) as I did not set it up the same way. If I recall, the linear regressor also utilized combined attributes etc. However, they are fairly close which is what I sort of suspected. I may try to play around if time permits after our discussion in lecture and try for Ridge as well. I did notice however that Lasso did not converge for lower values of alpha and that might be a negative for this algorithm. Also, I am not completely sure how well Lasso works with one hot vectors with many zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Read appendix B\n",
    "\n",
    "- Reflect on your last data project, read appendix B. Then, write down a few of the checklist items that your last data project could have used. If you have not yet done a data project, then write down a few of the items that you found most interesting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project checklist is most definitely something I would print out and lay beside me as I have a tendency to dive in without adequate prepping. For the most part,  I have done the majority of what was described in the text. However, I could place more emphasis on feature engineering and analyzing correlations between the new features. This requires a bit more expertise in the subject area and I suspect that I will generally evolve in that aspect. Similarly, I will need to learn more about models in various categories in order to perform 'quick and dirty' analysis on standard parameters. This is fairly important in order to iron down the exact models I would like to data experiment with or to build ensemble models of. Finally, I think the section on maintaining the solution is important as we sometimes do not pay enough attention to data rot or diminishing quality. Similar to any software system with scripts to automate tracking of memory leaks or data locks, we need to monitor the ML solution.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
